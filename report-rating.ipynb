{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shishirkulal/report-rating?scriptVersionId=286774421\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:08:36.050624Z","iopub.execute_input":"2025-12-17T07:08:36.051211Z","iopub.status.idle":"2025-12-17T07:08:36.323266Z","shell.execute_reply.started":"2025-12-17T07:08:36.05118Z","shell.execute_reply":"2025-12-17T07:08:36.322501Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/asap-2-0/ASAP2_train_sourcetexts.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import Dataset\n\n# 1. Load and Prepare Data\ndf = pd.read_csv('/kaggle/input/asap-2-0/ASAP2_train_sourcetexts.csv')\n\n# Ensure the columns match your CSV; usually 'score' and 'full_text'\n# We normalize scores to a 0-1 range for a regression task\nmax_val = df['score'].max()\ndf['label'] = df['score'].astype(float) / max_val\n\ntrain_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n\n# 2. Faster Model Choice (DistilRoBERTa)\nmodel_ckpt = \"distilbert/distilroberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n# Reducing max_length to 256 speeds up training significantly\ndef tokenize_func(batch):\n    return tokenizer(batch[\"full_text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ntrain_ds = Dataset.from_pandas(train_df[['full_text', 'label']]).map(tokenize_func, batched=True)\nval_ds = Dataset.from_pandas(val_df[['full_text', 'label']]).map(tokenize_func, batched=True)\n\n# 3. Load Model and Freeze early layers to save time\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=1)\n\n# Freeze all layers except the last 2 and the classifier head\nfor param in model.roberta.embeddings.parameters():\n    param.requires_grad = False\nfor layer in model.roberta.encoder.layer[:-2]:\n    for param in layer.parameters():\n        param.requires_grad = False\n\n# 4. Optimized Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./fast_report_rater\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"no\",             # Don't save checkpoints during training to save disk speed\n    learning_rate=5e-5,\n    per_device_train_batch_size=16, # Increased batch size for speed\n    num_train_epochs=2,             # 2 epochs is usually enough for a rater\n    weight_decay=0.01,\n    fp16=True,                      # Use GPU acceleration (Mixed Precision)\n    report_to=\"none\",\n    logging_steps=50\n)\n\n# 5. Trainer Setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n)\n\n# 6. Execute Training\nprint(\"Starting fast training...\")\ntrainer.train()\n\n# 7. Save for later use\nmodel.save_pretrained(\"./final_rater_model\")\ntokenizer.save_pretrained(\"./final_rater_model\")\nprint(\"Model saved to './final_rater_model'\")\n\n# --- QUICK PREDICTION FUNCTION ---\n\ndef get_report_rating(text):\n    model.eval()\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(model.device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Scale back to original score range\n    prediction = torch.sigmoid(outputs.logits).item()\n    return round(prediction * max_val, 2)\n\n# Test it\n# print(f\"Sample Rating: {get_report_rating('The project analyzes the impact of renewable energy...')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:14:30.416231Z","iopub.execute_input":"2025-12-17T07:14:30.416867Z","iopub.status.idle":"2025-12-17T07:21:04.060485Z","shell.execute_reply.started":"2025-12-17T07:14:30.416837Z","shell.execute_reply":"2025-12-17T07:21:04.059743Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1619574695f4fec98165cb49a1cfa5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fda43564cff416488d26ba69d2db003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbc3d8f1d16d4195845045f3d6a26ac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00f4be09a205461bb0720439aabc257a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6fb222c49a415ba54d6d308c270fa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21018 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70aaf70584224ef69b3f4ae301e1b1c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3710 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d49c3593d4f4bd6aafcb94858fc32d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9a4137e3094e7cb244570b69ea1914"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting fast training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 06:12, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.018900</td>\n      <td>0.016157</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.017400</td>\n      <td>0.016629</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Model saved to './final_rater_model'\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install textstat language_tool_python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:23:22.647893Z","iopub.execute_input":"2025-12-17T07:23:22.648232Z","iopub.status.idle":"2025-12-17T07:23:30.003366Z","shell.execute_reply.started":"2025-12-17T07:23:22.648204Z","shell.execute_reply":"2025-12-17T07:23:30.002426Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting textstat\n  Downloading textstat-0.7.12-py3-none-any.whl.metadata (15 kB)\nCollecting language_tool_python\n  Downloading language_tool_python-3.1.0-py3-none-any.whl.metadata (17 kB)\nCollecting pyphen (from textstat)\n  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from language_tool_python) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from language_tool_python) (4.67.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from language_tool_python) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from language_tool_python) (5.9.5)\nRequirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from language_tool_python) (0.10.2)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (2025.11.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->language_tool_python) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->language_tool_python) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->language_tool_python) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->language_tool_python) (2025.11.12)\nDownloading textstat-0.7.12-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.6/176.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading language_tool_python-3.1.0-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyphen, textstat, language_tool_python\nSuccessfully installed language_tool_python-3.1.0 pyphen-0.17.2 textstat-0.7.12\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport textstat\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# 1. Load the model you just trained\nmodel_path = \"./final_rater_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nmodel.eval() # Set to evaluation mode\n\ndef analyze_report(text):\n    # --- Part A: Scoring (The AI Model) ---\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Scale score to 0-100 (adjust max_val if your dataset max was different)\n    max_val = 10 \n    raw_prediction = torch.sigmoid(outputs.logits).item()\n    ai_score = round(raw_prediction * 100, 1)\n\n    # --- Part B: Generating Suggestions (Rule-based) ---\n    suggestions = []\n    \n    # Check 1: Length\n    word_count = len(text.split())\n    if word_count < 300:\n        suggestions.append(\"ğŸ“ **Increase Depth:** Your report is quite short. High-scoring reports usually provide more detailed evidence and analysis.\")\n    \n    # Check 2: Readability\n    read_score = textstat.flesch_reading_ease(text)\n    if read_score < 40:\n        suggestions.append(\"ğŸ“ **Simplify Language:** Your 'Reading Ease' score is low. Try breaking down long, complex sentences to make your points clearer.\")\n    elif read_score > 80:\n        suggestions.append(\"ğŸ“ **Professional Tone:** The language is very simple. Consider using more technical or academic vocabulary relevant to the project.\")\n\n    # Check 3: Structural Keywords\n    keywords = [\"conclusion\", \"references\", \"methodology\", \"introduction\"]\n    missing = [k for k in keywords if k not in text.lower()]\n    if missing:\n        suggestions.append(f\"ğŸ“ **Improve Structure:** Your report seems to be missing clear sections for: {', '.join(missing)}.\")\n\n    return {\n        \"Score\": ai_score,\n        \"Suggestions\": suggestions if suggestions else [\"Perfect! Your report meets all structural and readability standards.\"]\n    }\n\n# --- TEST IT ---\nsample_text = \"Your project report text goes here...\"\nresults = analyze_report(sample_text)\n\nprint(f\"--- REPORT CARD ---\")\nprint(f\"Final Grade: {results['Score']}/100\")\nprint(\"\\nHow to improve:\")\nfor s in results['Suggestions']:\n    print(s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:23:46.667844Z","iopub.execute_input":"2025-12-17T07:23:46.668718Z","iopub.status.idle":"2025-12-17T07:23:49.581085Z","shell.execute_reply.started":"2025-12-17T07:23:46.668669Z","shell.execute_reply":"2025-12-17T07:23:49.580319Z"}},"outputs":[{"name":"stdout","text":"--- REPORT CARD ---\nFinal Grade: 60.6/100\n\nHow to improve:\nğŸ“ **Increase Depth:** Your report is quite short. High-scoring reports usually provide more detailed evidence and analysis.\nğŸ“ **Professional Tone:** The language is very simple. Consider using more technical or academic vocabulary relevant to the project.\nğŸ“ **Improve Structure:** Your report seems to be missing clear sections for: conclusion, references, methodology, introduction.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport textstat\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, clear_output\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# 1. Load the model and tokenizer from your saved path\nMODEL_PATH = \"./final_rater_model\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\nmodel.eval()\n\n# 2. Define the Prediction & Logic Function\ndef analyze_report(text):\n    if len(text.strip()) < 20:\n        return \"Please enter a valid report (too short).\"\n\n    # --- AI Scoring ---\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Assuming training normalized scores to 0-1, we scale to 100\n    raw_prediction = torch.sigmoid(outputs.logits).item()\n    ai_score = round(raw_prediction * 100, 1)\n\n    # --- Suggestion Engine ---\n    suggestions = []\n    \n    # Word Count Check\n    words = text.split()\n    if len(words) < 250:\n        suggestions.append(\"âŒ <b>Depth:</b> Your report is very short. Expand your arguments with data or examples.\")\n    \n    # Readability Check\n    read_score = textstat.flesch_reading_ease(text)\n    if read_score < 45:\n        suggestions.append(\"âŒ <b>Readability:</b> Sentences are too complex. Try using shorter sentences to explain your points.\")\n    \n    # Structure Check\n    structure_points = [\"introduction\", \"conclusion\", \"method\", \"result\"]\n    missing = [p for p in structure_points if p not in text.lower()]\n    if missing:\n        suggestions.append(f\"âŒ <b>Structure:</b> You might be missing these key sections: {', '.join(missing)}.\")\n\n    # Format the Output\n    suggestion_html = \"\".join([f\"<li>{s}</li>\" for s in suggestions])\n    if not suggestions:\n        suggestion_html = \"<li>âœ… Great job! Your report is well-structured and readable.</li>\"\n\n    return ai_score, suggestion_html\n\n# 3. Create UI Elements\ntext_area = widgets.Textarea(\n    placeholder='Paste your project report here...',\n    description='Report:',\n    layout={'height': '300px', 'width': '100%'}\n)\n\nbutton = widgets.Button(\n    description='Rate My Report',\n    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n    layout={'width': 'max-content'}\n)\n\noutput = widgets.Output()\n\n# 4. Button Click Event\ndef on_button_clicked(b):\n    with output:\n        clear_output()\n        report_content = text_area.value\n        if not report_content:\n            print(\"Please paste a report first!\")\n            return\n            \n        print(\"Analyzing... please wait.\")\n        score, feedback = analyze_report(report_content)\n        \n        # Display Results in HTML for a nice look\n        display(HTML(f\"\"\"\n            <div style=\"border: 2px solid #4CAF50; padding: 20px; border-radius: 10px; background-color: #blue;\">\n                <h2 style=\"color: #4CAF50; margin-top: 0;\">Analysis Result</h2>\n                <div style=\"font-size: 24px; font-weight: bold;\">\n                    Overall Score: <span style=\"color: #2196F3;\">{score}/100</span>\n                </div>\n                <hr>\n                <h4 style=\"margin-bottom: 5px;\">Suggestions for Improvement:</h4>\n                <ul style=\"line-height: 1.6;\">\n                    {feedback}\n                </ul>\n            </div>\n        \"\"\"))\n\nbutton.on_click(on_button_clicked)\n\n# 5. Display the UI\ndisplay(HTML(\"<h3>ğŸš€ Project Report Rater (AI Powered)</h3>\"))\ndisplay(text_area, button, output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:27:40.08269Z","iopub.execute_input":"2025-12-17T07:27:40.083005Z","iopub.status.idle":"2025-12-17T07:27:40.269806Z","shell.execute_reply.started":"2025-12-17T07:27:40.08296Z","shell.execute_reply":"2025-12-17T07:27:40.269269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>ğŸš€ Project Report Rater (AI Powered)</h3>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Textarea(value='', description='Report:', layout=Layout(height='300px', width='100%'), placeholder='Paste yourâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee1e56397b741c599e9bb5fefb2532e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='success', description='Rate My Report', layout=Layout(width='max-content'), style=ButtonSâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1038635169a44e979346a34f39bad262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7900c335d744746b4301ca97fdbf758"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"!pip install textstat docx2txt PyPDF2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:27:55.464122Z","iopub.execute_input":"2025-12-17T07:27:55.464937Z","iopub.status.idle":"2025-12-17T07:27:59.182155Z","shell.execute_reply.started":"2025-12-17T07:27:55.464895Z","shell.execute_reply":"2025-12-17T07:27:59.181456Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: textstat in /usr/local/lib/python3.12/dist-packages (0.7.12)\nCollecting docx2txt\n  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\nCollecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: pyphen in /usr/local/lib/python3.12/dist-packages (from textstat) (0.17.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (4.67.1)\nDownloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: docx2txt, PyPDF2\nSuccessfully installed PyPDF2-3.0.1 docx2txt-0.9\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import io\nimport docx2txt\nimport PyPDF2\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, clear_output\n\n# --- File Extraction Helper ---\ndef extract_text_from_upload(uploaded_file):\n    content = uploaded_file['content']\n    filename = uploaded_file['name']\n    \n    if filename.endswith('.txt'):\n        return content.decode('utf-8')\n    \n    elif filename.endswith('.docx'):\n        # Use BytesIO to treat the uploaded bytes as a file\n        return docx2txt.process(io.BytesIO(content))\n    \n    elif filename.endswith('.pdf'):\n        pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))\n        text = \"\"\n        for page in pdf_reader.pages:\n            text += page.extract_text()\n        return text\n    return None\n\n# --- UI Setup ---\nuploader = widgets.FileUpload(\n    accept='.txt,.pdf,.docx',  # Only allow these types\n    multiple=False \n)\n\noutput_area = widgets.Output()\n\ndef on_upload_change(change):\n    with output_area:\n        clear_output()\n        if not uploader.value:\n            return\n        \n        # Get the first file from the upload list\n        uploaded_file = uploader.value[0]\n        print(f\"Reading: {uploaded_file['name']}...\")\n        \n        text = extract_text_from_upload(uploaded_file)\n        \n        if text:\n            # Call your analyze_report function from the previous step\n            score, feedback = analyze_report(text)\n            \n            display(HTML(f\"\"\"\n                <div style=\"border: 2px solid #2196F3; padding: 15px; border-radius: 10px;\">\n                    <h3 style=\"color: #2196F3;\">Analysis for: {uploaded_file['name']}</h3>\n                    <p style=\"font-size: 20px;\"><b>AI Score: {score}/100</b></p>\n                    <ul>{feedback}</ul>\n                </div>\n            \"\"\"))\n        else:\n            print(\"Unsupported file format.\")\n\nuploader.observe(on_upload_change, names='value')\n\ndisplay(HTML(\"<h4>ğŸ“ Upload your Project Report (.txt, .pdf, .docx)</h4>\"))\ndisplay(uploader, output_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:28:06.362239Z","iopub.execute_input":"2025-12-17T07:28:06.362577Z","iopub.status.idle":"2025-12-17T07:28:06.464328Z","shell.execute_reply.started":"2025-12-17T07:28:06.362542Z","shell.execute_reply":"2025-12-17T07:28:06.463763Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h4>ğŸ“ Upload your Project Report (.txt, .pdf, .docx)</h4>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='.txt,.pdf,.docx', description='Upload')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a73613d5c546dfbb166f6ff97d21f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6934905ab6474df4a86abf0c320d0418"}},"metadata":{}}],"execution_count":12}]}